{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Cleaning Basics**"
      ],
      "metadata": {
        "id": "A6ndVuGRkL8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the text corpus is given to us, it may have following issues:\n",
        "*   HTML tags\n",
        "*   Upper / Lower Case inconsistency\n",
        "*   Punctuations\n",
        "*   Stop words\n",
        "*  Words not in their root form\n",
        "*   And so on.. .\n",
        "Before using the data for predictions, we need to clean it."
      ],
      "metadata": {
        "id": "D3zsTWQLkPo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) HTML Tags removal:\n",
        "\n",
        "While scraping the text data from a website, you may get HTML tags included, so it is recommended that we remove them."
      ],
      "metadata": {
        "id": "MGkMAkiFkx7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "#Example:\n",
        "\n",
        "\"The market extended gains for the seventh consecutive session, climbing 1 percent to end at <b> record </b> closing high on May 31. Reliance Industries <h2> continued to be a leader </h2> in the rally, followed by <br> private banks & financials and FMCG stocks.\"\n",
        "\n",
        "To clean the above text, let us remove the words which are present in between the angle brackets ‘<’ , ‘>’. We need to write regex (regular expressions) for it.\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ynClrhc6k6k7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "W2EV4YLBj7FX",
        "outputId": "762330ea-297c-4d17-ae7a-1d1c1bf885a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The market extended gains for the seventh consecutive session, climbing 1 percent to end at  record  closing high on May 31. Reliance Industries  continued to be a leader  in the rally, followed by  private banks & financials and FMCG stocks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import re\n",
        "text_data = '''The market extended gains for the seventh consecutive session, climbing 1 percent to end at <b> record </b> closing high on May 31. Reliance Industries <h2> continued to be a leader </h2> in the rally, followed by <br> private banks & financials and FMCG stocks.'''\n",
        "html_pattern = re.compile('<.*?>')\n",
        "text_data = re.sub(html_pattern, '', text_data)\n",
        "text_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can notice that html tags have been replaced with empty strings."
      ],
      "metadata": {
        "id": "Vz6KesINl5Mt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Upper and lower case inconsistency:**\n",
        "\n",
        "Let us remove this inconsistency and convert everything into lower case.\n",
        "\n"
      ],
      "metadata": {
        "id": "_ZPW9LfmmkKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = text_data.lower()\n",
        "text_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "SNDXo8M-kw4i",
        "outputId": "915f0c11-9bee-4dc0-ba57-2df75a0c6528"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the market extended gains for the seventh consecutive session, climbing 1 percent to end at  record  closing high on may 31. reliance industries  continued to be a leader  in the rally, followed by  private banks & financials and fmcg stocks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) Remove Punctuations:**\n",
        "\n",
        "Punctuations in the text do not make much sense hence we can remove them.\n",
        "\n",
        "Example: % ^ & * , ) } etc"
      ],
      "metadata": {
        "id": "Kr1zt803m2uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = re.sub(r'[^\\w\\s]', '', text_data)\n",
        "text_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "9tsVCyKumx_7",
        "outputId": "c5307412-1e98-4ec1-db95-7ec12beb255f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the market extended gains for the seventh consecutive session climbing 1 percent to end at  record  closing high on may 31 reliance industries  continued to be a leader  in the rally followed by  private banks  financials and fmcg stocks'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4) Remove words having length less than or equal to 2:**\n",
        "\n",
        "Words that provide meaningful information often have word length more than 2."
      ],
      "metadata": {
        "id": "ddMb-baonNPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = ' '.join(word for word in text_data.split() if len(word)>2)\n",
        "text_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bWxgD6DxnQ7Y",
        "outputId": "a8f2b762-945a-4db8-e6ba-76ca4f35b092"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the market extended gains for the seventh consecutive session climbing percent end record closing high may reliance industries continued leader the rally followed private banks financials and fmcg stocks'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**\n",
        "\n",
        "Process of splitting the text, phrases, sentences into smaller units is called Tokenization.\n",
        "\n",
        "***Example:***\n",
        "\n",
        "Splitting of a text into sentences (Sentence is considered as token)\n",
        "Splitting of a sentence into words (Word is considered as token)\n",
        "We can import different types of tokenizers from the nltk library accordingly.\n"
      ],
      "metadata": {
        "id": "fQgey-Dgnezv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Sentence Tokenizer:**\n",
        "\n",
        "\n",
        "Text data will be splitted into sentences"
      ],
      "metadata": {
        "id": "WLf2au-2nove"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9EmxJhVn9JO",
        "outputId": "04438bbc-d915-415d-dd71-4c1e6d0f5d24"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text_data = 'The market extended gains for the seventh consecutive session, climbing 1 percent to end at record closing high on May 31. Reliance Industries continued to be a leader in the rally, followed by private banks & financials and FMCG stocks.'\n",
        "sent_tokenize(text_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4sih2V9nzet",
        "outputId": "37689b5e-83e7-446a-a0f8-8c658549f184"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The market extended gains for the seventh consecutive session, climbing 1 percent to end at record closing high on May 31.',\n",
              " 'Reliance Industries continued to be a leader in the rally, followed by private banks & financials and FMCG stocks.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Word Tokenizer:**\n",
        "\n",
        "\n",
        "Text data will be splitted into words"
      ],
      "metadata": {
        "id": "A6AI2y5fnyiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text_data = 'The market extended gains for the seventh consecutive session, climbing 1 percent to end at record closing high on May 31. Reliance Industries continued to be a leader in the rally, followed by private banks & financials and FMCG stocks.'\n",
        "tokenized_text= word_tokenize(text_data)\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10wJiW4kodsz",
        "outputId": "166027fb-c53f-4c04-d650-658fbbf9655f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'market', 'extended', 'gains', 'for', 'the', 'seventh', 'consecutive', 'session', ',', 'climbing', '1', 'percent', 'to', 'end', 'at', 'record', 'closing', 'high', 'on', 'May', '31', '.', 'Reliance', 'Industries', 'continued', 'to', 'be', 'a', 'leader', 'in', 'the', 'rally', ',', 'followed', 'by', 'private', 'banks', '&', 'financials', 'and', 'FMCG', 'stocks', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) WhitespaceTokenizer:**\n",
        "\n",
        "Based on white space, words are splitted. In Previous example, “,”, “.” are not part of the word, as they have their own usage and meaning, hence they are splitted separately and considered as separate tokens by their own. But in Whitespace tokenizer, characters which are occurring together will remain together."
      ],
      "metadata": {
        "id": "bP2bXviNnEwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "text_data = 'The market extended gains for the seventh consecutive session, climbing 1 percent to end at record closing high on May 31. Reliance Industries continued to be a leader in the rally, followed by private banks & financials and FMCG stocks.'\n",
        "print(WhitespaceTokenizer().tokenize(text_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAjrS7eaopKK",
        "outputId": "1dd50e72-6420-4620-ab0e-d2e7c7a7a842"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'market', 'extended', 'gains', 'for', 'the', 'seventh', 'consecutive', 'session,', 'climbing', '1', 'percent', 'to', 'end', 'at', 'record', 'closing', 'high', 'on', 'May', '31.', 'Reliance', 'Industries', 'continued', 'to', 'be', 'a', 'leader', 'in', 'the', 'rally,', 'followed', 'by', 'private', 'banks', '&', 'financials', 'and', 'FMCG', 'stocks.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stop word Removal**\n",
        "\n",
        "There are words in our sentences which do not provide any relevant information and hence they can be removed from the text.\n",
        "\n",
        "Example: and, of, at, it, the etc.\n",
        "\n",
        "There are multiple NLP libraries which operate on text and provide functionality to remove stop words. Some of the famous libraries that provide support for Stop word removal:\n",
        "*   NLTK\n",
        "*   Spacy\n",
        "*   Gensim\n",
        "\n",
        "If you do not have this library in your system, you can install it via below command:\n",
        "\n",
        "***pip install nltk***\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6m09ac0No1eA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVuiwz1JpSPT",
        "outputId": "778634da-d8dc-4cf4-a2e0-0d1eb988abe0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDu_Kjc_pin-",
        "outputId": "0bec67f0-9a20-435e-fbb4-af3cadedf1b6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(stopwords))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0L9PgO_pl6N",
        "outputId": "5ad52c27-a550-4b8f-ce8d-d882dc6ed197"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = 'The market extended gains for the seventh consecutive session, climbing 1 percent to end at record closing high on May 31. Reliance Industries continued to be a leader in the rally, followed by private banks & financials and FMCG stocks.'\n",
        "text_data = re.sub(r'[^\\w\\s]', '', text_data) #punctuation removal\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokenized_text = word_tokenize(text_data)\n",
        "tokenized_text = word_tokenize(text_data)\n",
        "print(tokenized_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "f-9e0p92p9GP",
        "outputId": "5afba072-0209-4c4f-ab78-97ec32908b90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'market', 'extended', 'gains', 'for', 'the', 'seventh', 'consecutive', 'session', 'climbing', '1', 'percent', 'to', 'end', 'at', 'record', 'closing', 'high', 'on', 'May', '31', 'Reliance', 'Industries', 'continued', 'to', 'be', 'a', 'leader', 'in', 'the', 'rally', 'followed', 'by', 'private', 'banks', 'financials', 'and', 'FMCG', 'stocks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "removed_stop_words_list = [word for word in tokenized_text if word not in stopwords]\n",
        "print(removed_stop_words_list)"
      ],
      "metadata": {
        "id": "4quenzz1tU5M",
        "outputId": "c1b8be2b-e3a3-406d-d0e8-6bf7ee5980e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'market', 'extended', 'gains', 'seventh', 'consecutive', 'session', 'climbing', '1', 'percent', 'end', 'record', 'closing', 'high', 'May', '31', 'Reliance', 'Industries', 'continued', 'leader', 'rally', 'followed', 'private', 'banks', 'financials', 'FMCG', 'stocks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*It is clearly visible that 'the', 'for', 'and' have been removed from the text*\n",
        "\n",
        "\n",
        "**Important Point:** Let’s say there is some word which does not make sense in your domain and you want to remove it too. There is a way by which you can enhance your stop words list by adding this word into Stop words list and later you can apply the same step for removal.\n",
        "\n",
        "**Example:** ‘fmcg’ is a more common word in your domain so you want to remove it."
      ],
      "metadata": {
        "id": "9aZOf20JttDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.append('fmcg')\n",
        "len(stopwords)\n"
      ],
      "metadata": {
        "id": "9LfjB90It7K8",
        "outputId": "dbd745ce-4db1-4e84-a913-6a9c7bf957e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "180"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "removed_stop_words_list = [word for word in tokenized_text if word not in stopwords]\n",
        "print(removed_stop_words_list)\n"
      ],
      "metadata": {
        "id": "ZWFyy9ROuEiv",
        "outputId": "1d7470fc-69eb-4cfb-d7ab-5eae43ea5ae9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'market', 'extended', 'gains', 'seventh', 'consecutive', 'session', 'climbing', '1', 'percent', 'end', 'record', 'closing', 'high', 'May', '31', 'Reliance', 'Industries', 'continued', 'leader', 'rally', 'followed', 'private', 'banks', 'financials', 'FMCG', 'stocks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*At this point, we have cleaned data, however, there are some words which are not in their root form. And this problem can affect the model’s accuracy. Hence it is recommended to convert the words to their base forms. We are going to learn these techniques going forward*"
      ],
      "metadata": {
        "id": "nkFiyVV7qgt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stemming & Lemmatization**\n",
        "When we work on some text document, removal of punctuation and stop words are just not enough, there is still something more which needs our attention.\n",
        "\n",
        "The words that we use in sentences can take any form. Words can be used in present tense, or past or may be in future tense, accordingly the word will change."
      ],
      "metadata": {
        "id": "Jh50tdOgu8oz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For e.g.\n",
        "\n",
        "    - The word ‘Go’ is ‘Go’ / ‘Goes’ in present tense and ‘Went’ in past tense\n",
        "\n",
        "    - The word ‘See’ is ‘See’ / ‘Sees’ in present tense, whereas it is ‘Saw’ in past tense\n",
        "\n",
        "\n",
        "\n",
        "These inconsistencies in data can affect the model training and predictions, hence, we need to make sure that the words exist in their root forms.\n",
        "\n",
        "\n",
        "\n",
        "To handle this, there are two methods:"
      ],
      "metadata": {
        "id": "WD2sVUV5vGPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Stemming & Lemmatization\n",
        "When we work on some text document, removal of punctuation and stop words are just not enough, there is still something more which needs our attention.\n",
        "\n",
        "\n",
        "\n",
        "The words that we use in sentences can take any form. Words can be used in present tense, or past or may be in future tense, accordingly the word will change.\n",
        "\n",
        "\n",
        "\n",
        "For e.g.\n",
        "\n",
        "    - The word ‘Go’ is ‘Go’ / ‘Goes’ in present tense and ‘Went’ in past tense\n",
        "\n",
        "    - The word ‘See’ is ‘See’ / ‘Sees’ in present tense, whereas it is ‘Saw’ in past tense\n",
        "\n",
        "\n",
        "\n",
        "These inconsistencies in data can affect the model training and predictions, hence, we need to make sure that the words exist in their root forms.\n",
        "\n",
        "\n",
        "\n",
        "To handle this, there are two methods:\n",
        "\n",
        "\n",
        "\n",
        "**1) Stemming:**\n",
        "\n",
        "\n",
        "Stemming is the process of converting/reducing the inflected words to their root form. In this method, the suffixes are removed from the inflected word so that it becomes the root.\n",
        "\n",
        "For eg. From the word “Going”, “ing” suffix will get removed and the inflected word “Going” will become “Go” which is the root form.\n",
        "\n",
        "Few more examples:\n",
        "*   Developing -> Develop\n",
        "*   Developed -> Develop\n",
        "*   Development -> Develop\n",
        "*   Develops -> Develop"
      ],
      "metadata": {
        "id": "7nA3mwJpvMcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All these inflected words take their root form when their suffixes are removed. Internally the stemming process uses some rules for trimming the suffix part.\n",
        "\n",
        " We can implement stemming in Python using famous library called as “nltk”\n",
        "\n",
        "If you don't have nltk installed in your machine, you can simply type: ***pip install nltk***"
      ],
      "metadata": {
        "id": "BoFFj3_Kvqap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "print(porter.stem(\"developing\"))\n",
        "print(porter.stem(\"develops\"))\n",
        "print(porter.stem(\"development\"))\n",
        "print(porter.stem(\"developed\"))"
      ],
      "metadata": {
        "id": "mAQ2nUBTvZPk",
        "outputId": "ec152b34-e1bb-4093-f2a1-6bb33a824e2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "develop\n",
            "develop\n",
            "develop\n",
            "develop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, there are some words which do not get properly handled by the “Stemming” process.\n",
        "\n",
        "**For e.g.** “went”, “flew”, “saw” these words can’t be converted properly to their base forms if Stemming is applied."
      ],
      "metadata": {
        "id": "Mr2nsvzCzWZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(porter.stem(\"went\"))\n",
        "print(porter.stem(\"flew\"))\n",
        "print(porter.stem(\"saw\"))"
      ],
      "metadata": {
        "id": "AcG-LGA7uNJl",
        "outputId": "0906bccf-b850-46fd-9dae-e4374e3ce077",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "went\n",
            "flew\n",
            "saw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Surprisingly, there is no change in the output**\n",
        "because the Stemming process is not smart enough.\n",
        "\n",
        "It **just knows how to trim the suffix part**, but it does not know how to change the form of the word.\n",
        "\n",
        "To solve this issue, there should be some algorithm which understands the linguistic meaning of the sentence and converts each word to its base form accordingly.\n",
        "\n",
        "Fortunately we have **Lemmatization** for this work."
      ],
      "metadata": {
        "id": "S63MCY9yzfuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pros and Cons of Stemmer:**\n",
        "\n",
        "***Pros:***\n",
        "\n",
        "  Computationally Fast: As it simply trims the suffix without worrying about the context of word\n",
        "\n",
        "\n",
        "***Cons:***\n",
        "\n",
        "  It is not useful enough if you are concerned about the valid words. Stemmer can give you some words which do not have any meaning.\n",
        "\n",
        " **\"Goes” -> “goe”**"
      ],
      "metadata": {
        "id": "wDaj4wXA0RvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Lemmatization:**\n",
        "\n",
        "\n",
        "It is where the words are converted to their root forms by understanding the context of the word in the sentence.\n",
        "\n",
        "In Stemming, the root word which we get after conversion is called a **stem**.\n",
        "\n",
        "Whereas, it is called a **lemma** in Lemmatization."
      ],
      "metadata": {
        "id": "ZrxZlRhbzt81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pros:**\n",
        "  The root word which we get after conversion holds some meaning and the word belongs to the Dictionary.\n",
        "**Cons:**\n",
        "  It is computationally expensive.\n",
        "\n",
        "*NLTK provides a class called **WordNet** for this purpose.*"
      ],
      "metadata": {
        "id": "4kuVsbP70wuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "x0-r4T2f07ZT"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "print(wordnet_lemmatizer.lemmatize(\"going\"))\n",
        "print(wordnet_lemmatizer.lemmatize(\"goes\")) # Lemmatizer is able to convert it to \"go\"\n",
        "print(wordnet_lemmatizer.lemmatize(\"went\"))\n",
        "print(porter.stem(\"goes\")) # Stemming is unable to normalize the word \"goes\" properly"
      ],
      "metadata": {
        "id": "Hp3XQ-8B1HVs",
        "outputId": "15ebe9ff-b5e0-4813-abcf-168eac42b504",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "going\n",
            "go\n",
            "went\n",
            "goe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**But you might be wondering that Lemmatizer is unable to normalize the words “going” and “went” into their root forms.**"
      ],
      "metadata": {
        "id": "zQHzZyay1WBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is because we have not passed the context to it.\n",
        "\n",
        "Part of speech “pos” is the parameter which we need to specify. By default it is NOUN.\n",
        "\n",
        "If a word is a verb which we want to normalize, then we need to specify with the value as “v”"
      ],
      "metadata": {
        "id": "NQ4J6sOx1biq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "print(wordnet_lemmatizer.lemmatize(\"going\", pos=\"v\"))\n",
        "print(wordnet_lemmatizer.lemmatize(\"goes\", pos=\"v\"))\n",
        "print(wordnet_lemmatizer.lemmatize(\"went\", pos=\"v\"))\n",
        "print(wordnet_lemmatizer.lemmatize(\"go\", pos = \"v\"))\n",
        "print(wordnet_lemmatizer.lemmatize(\"studies\", pos = \"v\"))\n",
        "print(wordnet_lemmatizer.lemmatize(\"studying\", pos = \"v\"))\n",
        "print(wordnet_lemmatizer.lemmatize(\"studied\", pos = \"v\"))\n",
        "print(wordnet_lemmatizer.lemmatize(\"dogs\")) # by default, it is noun\n",
        "print(wordnet_lemmatizer.lemmatize(\"dogs\", pos=\"n\"))"
      ],
      "metadata": {
        "id": "2ar6qcWU1ryy",
        "outputId": "89d58b44-56b2-4aa3-a732-1e8e3a0feabf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "go\n",
            "go\n",
            "go\n",
            "go\n",
            "study\n",
            "study\n",
            "study\n",
            "dog\n",
            "dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So far we have looked into many text cleaning and normalization techniques**\n",
        "\n",
        "**Thank you for your active participation !!!**\n",
        "\n",
        "⭐⭐⭐⭐⭐"
      ],
      "metadata": {
        "id": "ipMghqGm1yq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qDaLVJfj1mBC"
      }
    }
  ]
}